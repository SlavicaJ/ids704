
<style>
.reveal section p {
  color: black;
  font-size: .7em;
  font-family: 'Helvetica'; #this is the font/color of text in slides
}


.section .reveal .state-background {
    background: white;}
.section .reveal h1,
.section .reveal p {
    color: black;
    position: relative;
    top: 4%;}



</style>



Basic Text Analysis
========================================================
author: Chris Bail 
date: Duke University
autosize: true
transition: fade  
  website: https://www.chrisbail.net  
  github: https://github.com/cbail  
  Twitter: https://www.twitter.com/chris_bail

========================================================

# **CLEANING TEXT**

Character Encoding
========================================================

Character Encoding
========================================================
&nbsp;
<img src="character-viewer.png" height="400" />


GREP
========================================================

GREP
========================================================
&nbsp;
```{r}
duke_web_scrape<- "Class of 2018: Senior Stories of Discovery, Learning and Serving\n\n\t\t\t\t\t\t\t" 
```

grepl
========================================================
&nbsp;
```{r}
grepl("Class", duke_web_scrape)
```


gsub
========================================================
&nbsp;
```{r}
gsub("\t", "", duke_web_scrape)
```

gsub (2 patterns)
========================================================
&nbsp;
```{r}
gsub("\t|\n", "", duke_web_scrape)
```

More GREP
========================================================
&nbsp;
```{r}
some_text<-c("This","Professor","is","not","so","great")
some_text[grep("^[P]", some_text)]
```

Regex Cheat Sheet
========================================================
&nbsp;

http://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf


Escaping Text
========================================================
&nbsp;
```{r, eval=FALSE}
text_chunk<-c("[This Professor is not so Great]")
gsub("\","", text_chunk)
```


Escaping Text
========================================================
&nbsp;
```{r}
text_chunk<-c("[This Professor is not so Great]")
gsub('\\[|\\]',"", text_chunk)
```

========================================================

# **UNITS OF ANALYSIS**

Tokenization
========================================================

Tokenization
========================================================

<img src="token.png" height="400" />

N-grams
========================================================


N-grams
========================================================

<img src="8ARA1.png" height="400" />

========================================================

# **CREATING TEXT DATABASES**

What is a Corpus?
========================================================


Creating a Corpus
========================================================
&nbsp;
```{r, eval=FALSE}
load(url("https://cbail.github.io/Trump_Tweets.Rdata"))
head(trumptweets$text)
```
```{r, echo=FALSE}
load("~/ids704/basic-text-analysis/Rpres/trumptweets.Rdata")
head(trumptweets$text)
```

Creating a Corpus
========================================================
&nbsp;
```{r, eval=FALSE}
install.packages("tm")
```

Creating a Corpus
========================================================
&nbsp;
```{r}
library(tm)
trump_corpus <- Corpus(VectorSource(as.vector(trumptweets$text))) 
trump_corpus
```

TidyText
========================================================




TidyText
========================================================
&nbsp;
```{r}

library(tidytext)
library(dplyr)
tidy_trump_tweets<- trumptweets %>%
    select(created_at,text) %>%
    unnest_tokens("word", text)
head(tidy_trump_tweets)
```

TidyText is... tidy
========================================================
&nbsp;
```{r}
tidy_trump_tweets %>%
  count(word) %>%
    arrange(desc(n))
```

Corpus vs. TidyText
========================================================

========================================================

# **TEXT PRE-PROCESSING**

Stopwords
========================================================


Stopwords
========================================================

In `tm`:

```{r}
trump_corpus <- tm_map(trump_corpus, removeWords, stopwords("english"))
```

Stopwords
========================================================

In `tidytext`:

```{r}
 data("stop_words")
    tidy_trump_tweets<-tidy_trump_tweets %>%
      anti_join(stop_words)
```

Inspect top words again
========================================================
&nbsp;

```{r}
tidy_trump_tweets %>%
  count(word) %>%
    arrange(desc(n))
```

Remove Punctuation
========================================================


Remove Punctuation
========================================================

In `tm`:

```{r}
trump_corpus <- tm_map(trump_corpus, content_transformer(removePunctuation))
```

(punctuation marks are removed automatically in `tidytext`)

Remove Numbers
========================================================

In `tm`:

```{r}
trump_corpus <- tm_map(trump_corpus, content_transformer(removeNumbers))
```

In `tidytext`:

```{r}
tidy_trump_tweets<-tidy_trump_tweets[-grep("\\b\\d+\\b", tidy_trump_tweets$word),]
```


Word Case
========================================================


Word Case
========================================================

In `tm`:

```{r}
trump_corpus <- tm_map(trump_corpus,  content_transformer(tolower)) 
```

Once again, `tidytext` does this for you.


Removing whitespaces
========================================================


Removing whitespaces
========================================================

In `tm`:

```{r}
trump_corpus <- tm_map(trump_corpus, content_transformer(stripWhitespace))
```

In `tidytext`:

```{r}
tidy_trump_tweets$word <- gsub("\\s+","",tidy_trump_tweets$word)
```

Stemming
========================================================

Stemming
========================================================

In `tm`:

```{r}
trump_corpus  <- tm_map(trump_corpus, content_transformer(stemDocument), language = "english")
```

In `tidytext`:
```{r}
library(SnowballC)
  tidy_trump_tweets<-tidy_trump_tweets %>%
      mutate_at("word", funs(wordStem((.), language="en")))
```

========================================================

# **THE DOCUMENT-TERM MATRIX**

The DTM
========================================================

In `tm`:

```{r}
trump_DTM <- DocumentTermMatrix(trump_corpus, control = list(wordLengths = c(2, Inf)))
inspect(trump_DTM[1:5,3:8])

```

The DTM
========================================================

In `tidytext`:

```{r}
tidy_trump_DTM<-
  tidy_trump_tweets %>%
  count(created_at, word) %>%
  cast_dtm(created_at, word, n)
```

========================================================

# **Class Exercise**

1) Pick one of the Amazon review datasets from this link: http://jmcauley.ucsd.edu/data/amazon/

2) Create a tidytext dataset of these reviews

3) Count the top five words in the dataset after removing stop words;

4) Create a Document-Term Matrix

